{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.book import *\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "#### In Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. (Various methods will be discussed in the live session.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Vocab Size of Texts 1-4 (16948, 6283, 2615, 9110)\n",
      "Normalized Vocab Size of Texts 1-4 [[0.83040416 0.30784926 0.12812762 0.44636428]]\n"
     ]
    }
   ],
   "source": [
    "# Took Q1 literally, and only counted text values. Converted everything to lower and then stripped out punctuation, numbers, et... anything that isn't alphabetic.\n",
    "# Create list to hold vocab size of multiple text strings\n",
    "vocab_size1 = len(set(word.lower() for word in text1 if word.isalpha()))\n",
    "vocab_size2 = len(set(word.lower() for word in text2 if word.isalpha()))\n",
    "vocab_size3 = len(set(word.lower() for word in text3 if word.isalpha()))\n",
    "vocab_size4 = len(set(word.lower() for word in text4 if word.isalpha()))\n",
    "vocab_to_norm = (vocab_size1,vocab_size2,vocab_size3,vocab_size4)\n",
    "print(\"Raw Vocab Size of Texts 1-4\", vocab_to_norm)\n",
    "\n",
    "# Normalize the data\n",
    "raw = np.array(vocab_to_norm)\n",
    "norm = preprocessing.normalize([raw])\n",
    "print(\"Normalized Vocab Size of Texts 1-4\", norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normaliztion is used to ensure that every feature has the importance in the model. If we leave the data in the default scale, some features will get weighted more than others that could be a problem when it comes to the modeling setps. I chose to normalize using the sklearn normalizer becuase it rescales the vector for each sample to have unit norm, independently of the distribution of the samples.   Source: \"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "#### After consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Long Word Vocab Size of Texts 1-4 (22, 4, 0, 17)\n",
      "Normalized Vocab Size of Texts 1-4 [[0.78322074 0.14240377 0.         0.60521603]]\n"
     ]
    }
   ],
   "source": [
    "# Took Q2 literally, and only counted text values. Converted everything to lower and then stripped out punctuation, numbers, etc... anything that isn't alphabetic.\n",
    "# Create list to hold vocab size of multiple text strings\n",
    "long_words1 = len(set(word.lower() for word in text1 if word.isalpha() and len(word) > 15))\n",
    "long_words2 = len(set(word.lower() for word in text2 if word.isalpha() and len(word) > 15))\n",
    "long_words3 = len(set(word.lower() for word in text3 if word.isalpha() and len(word) > 15))\n",
    "long_words4 = len(set(word.lower() for word in text4 if word.isalpha() and len(word) > 15))\n",
    "vocab_to_norm_long = (long_words1,long_words2,long_words3,long_words4)\n",
    "print(\"Raw Long Word Vocab Size of Texts 1-4\", vocab_to_norm_long)\n",
    "\n",
    "# Normalize the data\n",
    "raw_long = np.array(vocab_to_norm_long)\n",
    "norm_long = preprocessing.normalize([raw_long])\n",
    "print(\"Normalized Vocab Size of Texts 1-4\", norm_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normaliztion is used to ensure that every feature has the importance in the model. If we leave the data in the default scale, some features will get weighted more than others that could be a problem when it comes to the modeling setps. I chose to normalize using the sklearn normalizer becuase it rescales the vector for each sample to have unit norm, independently of the distribution of the samples.   Source: \"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "#### Now create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_div(text):\n",
    "    return (len(set(text)) / len(text))\n",
    "lex1 = lex_div(text1)\n",
    "lex2 = lex_div(text2)\n",
    "lex3 = lex_div(text3)\n",
    "lex4 = lex_div(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Difficulty Score Texts 1-4 (1.6876877558115633, 0.4985168655795573, 0.19043214893013166, 1.117756536221207)\n"
     ]
    }
   ],
   "source": [
    "text1_score = lex1 + np.take(norm,0) + np.take(norm_long,0)\n",
    "text2_score = lex2 + np.take(norm,1) + np.take(norm_long,1)\n",
    "text3_score = lex3 + np.take(norm,2) + np.take(norm_long,2)\n",
    "text4_score = lex4 + np.take(norm,3) + np.take(norm_long,3)\n",
    "text_diff_score = (text1_score,text2_score,text3_score,text4_score)\n",
    "print(\"Text Difficulty Score Texts 1-4\", text_diff_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are actually quite similar to the ones calculated in HW1, albeit on a different scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
