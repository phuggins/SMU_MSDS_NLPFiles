{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /srv/conda/envs/notebook/lib/python3.6/site-packages (3.6.2)\n",
      "Requirement already satisfied: click in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (4.61.1)\n",
      "Requirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: importlib-metadata in /srv/conda/envs/notebook/lib/python3.6/site-packages (from click->nltk) (3.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (3.7.4.3)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement string\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for string\u001b[0m\n",
      "Requirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.6/site-packages (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install string\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import numpy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_kernels\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tEvaluate text similarity of Amazon book search results by doing the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.\tDo a book search on Amazon. Manually copy the full book title (including subtitle) of each of the top 24 books listed in the first two pages of search results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did a simple search on Science Fiction and Fantasy books sorted by Best Selling\n",
    "books = ['The Midnight Library: A Novel', 'Project Hail Mary: A Novel', 'Van Richtens Guide to Ravenloft (Dungeons & Dragons)', 'The Invisible Life of Addie LaRue', 'Berserk Deluxe Volume 1', 'The Alchemist, 25th Anniversary: A Fable About Following Your Dream', 'Players Handbook (Dungeons & Dragons)', 'Go Tell the Bees That I Am Gone: A Novel (Outlander)', 'From Blood and Ash (Blood And Ash Series)', 'Tashas Cauldron of Everything (D&D Rules Expansion) (Dungeons & Dragons)', 'Berserk Deluxe Volume 2', '1984 (Signet Classics), Book Cover May Vary', 'Klara and the Sun: A novel', 'Invincible Compendium Volume 2', 'A Court of Thorns and Roses', 'The Wild Beyond the Witchlight: A Feywild Adventure (Dungeons & Dragons-Abenteuerbuch) - Englische Version', 'Berserk Deluxe Volume 3', 'Dungeons & Dragons Dungeon Masters Guide (Core Rulebook, D&D Roleplaying Game)', 'Dungeons & Dragons Monster Manual (Core Rulebook, D&D Roleplaying Game)', 'Fahrenheit 451', 'A Touch of Malice (Hades X Persephone)', 'House in the Cerulean Sea', 'Xanathars Guide to Everything (Dungeons & Dragons)', 'The Lost Apothecary: A Novel', 'The Handmaids Tale', 'A Court of Silver Flames (A Court of Thorns and Roses, 5)', 'The Atlas Six', 'Dune (Dune Chronicles, Book 1)', 'Volos Guide to Monsters (Dungeons & Dragons)', 'Strixhaven: Curriculum of Chaos (D&D/MTG Adventure Book)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.\tIn Python, run one of the text-similarity measures covered in this course, e.g., cosine similarity. Compare each of the book titles, pairwise, to every other one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.15952759 0.         0.08140086 0.         0.06151969\n",
      "  0.         0.17034445 0.         0.         0.         0.\n",
      "  0.2592575  0.         0.         0.113818   0.         0.\n",
      "  0.         0.         0.         0.08548902 0.         0.28447125\n",
      "  0.11693528 0.         0.11693528 0.         0.         0.        ]\n",
      " [0.15952759 1.         0.         0.         0.         0.\n",
      "  0.         0.09552684 0.         0.         0.         0.\n",
      "  0.14538806 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.15952759\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.19506923 0.         0.         0.1317442  0.         0.\n",
      "  0.         0.         0.         0.1032293  0.         0.22720357\n",
      "  0.13238474 0.         0.         0.         0.45276743 0.\n",
      "  0.         0.         0.         0.         0.44085679 0.        ]\n",
      " [0.08140086 0.         0.         1.         0.         0.04724867\n",
      "  0.         0.04874371 0.         0.08329478 0.         0.\n",
      "  0.074186   0.         0.11025293 0.08741508 0.         0.\n",
      "  0.         0.         0.09641748 0.06565771 0.         0.08140086\n",
      "  0.08980923 0.13466282 0.08980923 0.         0.         0.08365599]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.         0.25815497 0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.06151969 0.         0.         0.04724867 0.         1.\n",
      "  0.         0.03683866 0.         0.         0.         0.\n",
      "  0.05606697 0.         0.         0.06606501 0.         0.\n",
      "  0.         0.         0.         0.04962162 0.         0.06151969\n",
      "  0.06787442 0.         0.06787442 0.         0.         0.        ]\n",
      " [0.         0.         0.19506923 0.         0.         0.\n",
      "  1.         0.         0.         0.17786795 0.         0.\n",
      "  0.         0.         0.         0.13936996 0.         0.17067023\n",
      "  0.17873275 0.         0.         0.         0.22427592 0.\n",
      "  0.         0.         0.         0.         0.21837605 0.        ]\n",
      " [0.17034445 0.09552684 0.         0.04874371 0.         0.03683866\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.15524619 0.         0.         0.06815545 0.         0.\n",
      "  0.         0.         0.         0.05119175 0.         0.17034445\n",
      "  0.07002211 0.         0.07002211 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.17736766 0.         0.17652645 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.10780461 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.1317442  0.08329478 0.         0.\n",
      "  0.17786795 0.         0.         1.         0.         0.\n",
      "  0.         0.         0.09837181 0.0941265  0.         0.11526581\n",
      "  0.120711   0.         0.0860273  0.         0.31987065 0.\n",
      "  0.         0.12015123 0.         0.         0.14748496 0.07464102]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.         0.25815497 0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.10804204 0.         0.105847  ]\n",
      " [0.2592575  0.14538806 0.         0.074186   0.         0.05606697\n",
      "  0.         0.15524619 0.17736766 0.         0.         0.\n",
      "  1.         0.         0.16860807 0.10372989 0.         0.\n",
      "  0.         0.         0.         0.07791181 0.         0.2592575\n",
      "  0.10657087 0.10296886 0.10657087 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.25815497 0.\n",
      "  0.         0.         0.         0.         0.25815497 0.\n",
      "  0.         1.         0.         0.         0.25815497 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.11025293 0.         0.\n",
      "  0.         0.         0.17652645 0.09837181 0.         0.\n",
      "  0.16860807 0.         1.         0.         0.         0.\n",
      "  0.         0.         0.11386983 0.         0.         0.\n",
      "  0.         0.83311845 0.         0.         0.         0.09879841]\n",
      " [0.113818   0.         0.1032293  0.08741508 0.         0.06606501\n",
      "  0.13936996 0.06815545 0.         0.0941265  0.         0.\n",
      "  0.10372989 0.         0.         1.         0.         0.09031751\n",
      "  0.09458414 0.         0.         0.09180529 0.11868528 0.113818\n",
      "  0.12557493 0.         0.12557493 0.         0.11556311 0.10510194]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.         0.25815497 0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.22720357 0.         0.         0.\n",
      "  0.17067023 0.         0.         0.11526581 0.         0.\n",
      "  0.         0.         0.         0.09031751 0.         1.\n",
      "  0.63092019 0.         0.         0.         0.26122156 0.\n",
      "  0.         0.         0.         0.         0.25434979 0.        ]\n",
      " [0.         0.         0.13238474 0.         0.         0.\n",
      "  0.17873275 0.         0.         0.120711   0.         0.\n",
      "  0.         0.         0.         0.09458414 0.         0.63092019\n",
      "  1.         0.         0.         0.         0.15220601 0.\n",
      "  0.         0.         0.         0.         0.14820204 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.09641748 0.         0.\n",
      "  0.         0.         0.         0.0860273  0.         0.\n",
      "  0.         0.         0.11386983 0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.13908049 0.         0.         0.         0.08640036]\n",
      " [0.08548902 0.         0.         0.06565771 0.         0.04962162\n",
      "  0.         0.05119175 0.         0.         0.         0.\n",
      "  0.07791181 0.         0.         0.09180529 0.         0.\n",
      "  0.         0.         0.         1.         0.         0.08548902\n",
      "  0.09431968 0.         0.09431968 0.         0.         0.        ]\n",
      " [0.         0.         0.45276743 0.         0.         0.\n",
      "  0.22427592 0.         0.         0.31987065 0.         0.\n",
      "  0.         0.         0.         0.11868528 0.         0.26122156\n",
      "  0.15220601 0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.50686397 0.        ]\n",
      " [0.28447125 0.15952759 0.         0.08140086 0.         0.06151969\n",
      "  0.         0.17034445 0.         0.         0.         0.\n",
      "  0.2592575  0.         0.         0.113818   0.         0.\n",
      "  0.         0.         0.         0.08548902 0.         1.\n",
      "  0.11693528 0.         0.11693528 0.         0.         0.        ]\n",
      " [0.11693528 0.         0.         0.08980923 0.         0.06787442\n",
      "  0.         0.07002211 0.         0.         0.         0.\n",
      "  0.10657087 0.         0.         0.12557493 0.         0.\n",
      "  0.         0.         0.         0.09431968 0.         0.11693528\n",
      "  1.         0.         0.12901422 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.13466282 0.         0.\n",
      "  0.         0.         0.10780461 0.12015123 0.         0.\n",
      "  0.10296886 0.         0.83311845 0.         0.         0.\n",
      "  0.         0.         0.13908049 0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.12067228]\n",
      " [0.11693528 0.         0.         0.08980923 0.         0.06787442\n",
      "  0.         0.07002211 0.         0.         0.         0.\n",
      "  0.10657087 0.         0.         0.12557493 0.         0.\n",
      "  0.         0.         0.         0.09431968 0.         0.11693528\n",
      "  0.12901422 0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.10804204\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.11481162]\n",
      " [0.         0.         0.44085679 0.         0.         0.\n",
      "  0.21837605 0.         0.         0.14748496 0.         0.\n",
      "  0.         0.         0.         0.11556311 0.         0.25434979\n",
      "  0.14820204 0.         0.         0.         0.50686397 0.\n",
      "  0.         0.         0.         0.         1.         0.        ]\n",
      " [0.         0.         0.         0.08365599 0.         0.\n",
      "  0.         0.         0.         0.07464102 0.         0.105847\n",
      "  0.         0.         0.09879841 0.10510194 0.         0.\n",
      "  0.         0.         0.08640036 0.         0.         0.\n",
      "  0.         0.12067228 0.         0.11481162 0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer()\n",
    "\n",
    "values = books\n",
    "X_train_counts = tfidf_vectorizer.fit_transform(values)\n",
    "similarities = cosine_similarity(X_train_counts) \n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.\tWhich two titles are the most similar to each other? Which are the most dissimilar? Where do they rank, among the first 24 results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest Cosine Similarity: 0.8331184490542072\n",
      "Location in Array (Largest): (array([14, 25]), array([25, 14]))\n",
      "Most Similar Titles: A Court of Thorns and Roses , A Court of Silver Flames (A Court of Thorns and Roses, 5)\n",
      "\n",
      "\n",
      "Smallest Cosine Similarity (Excluding 0's): 0.03683865537162722\n",
      "Location in Array (Smallest): (array([5, 7]), array([7, 5]))\n",
      "Least Similar Titles: The Alchemist, 25th Anniversary: A Fable About Following Your Dream , Go Tell the Bees That I Am Gone: A Novel (Outlander)\n"
     ]
    }
   ],
   "source": [
    "maxElement = numpy.amax(similarities[similarities < 1])\n",
    "print(\"Largest Cosine Similarity: \" + str(maxElement))\n",
    "\n",
    "result = numpy.where(similarities == numpy.amax(similarities[similarities < 1]))\n",
    "print(\"Location in Array (Largest): \" + str(result))\n",
    "print(\"Most Similar Titles: \" + books[14] + \" , \" + books[25])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "maxElement = numpy.amin(similarities[similarities > 0])\n",
    "print(\"Smallest Cosine Similarity (Excluding 0's): \" + str(maxElement))\n",
    "\n",
    "result = numpy.where(similarities == numpy.amin(similarities[similarities > 0]))\n",
    "print(\"Location in Array (Smallest): \" + str(result))\n",
    "print(\"Least Similar Titles: \" + books[5] + \" , \" + books[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Now evaluate using a major search engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Enter one of the book titles from question 1a into Google, Bing, or Yahoo!. Copy the capsule of the first organic result and the 20th organic result. Take web results only (i.e., not video results), and skip sponsored results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "OriginalTitle = \"The Midnight Library: A Novel\"\n",
    "FirstOR = \"The Midnight Library by Matt Haig - Goodreads\"\n",
    "TwentythOR = \"The Midnight Library: A Novel | theproread.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Run the same text similarity calculation that you used for question 1b on each of these capsules in comparison to the original query (book title). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36544885 0.66952061]\n",
      " [0.36544885 1.         0.24467553]\n",
      " [0.66952061 0.24467553 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "organics = [\"The Midnight Library: A Novel\", \"The Midnight Library by Matt Haig - Goodreads\", \"The Midnight Library: A Novel | theproread.com\"]\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer()\n",
    "\n",
    "values = organics\n",
    "X_train_counts = tfidf_vectorizer.fit_transform(values)\n",
    "similarities = cosine_similarity(X_train_counts) \n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.Which one has the highest similarity measure? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "#### The 20th organic result was actually a closer match than the 1st result!\n",
    "#### This was a really interesting assignment... generally I don't go past the 1st or 2nd page of the search results but this would clearly show that it's not always the 1st result that is the best. I was not surprising that the cosine similarity was the highest when the book titles were nearly exactly the same. I can see how this would be extremely helpful in large scale scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
